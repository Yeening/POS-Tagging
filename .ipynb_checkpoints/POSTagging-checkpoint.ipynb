{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "[{'N': 'N'}, {'O': 'N'}, {'N': 'O', 'D': 'O', 'A': 'O'}, {'UNK': 'D'}, {'N': 'UNK'}, {'N': 'N'}, {'O': 'N'}, {'V': 'O'}, {'UNK': 'V'}, {'A': 'UNK', 'R': 'UNK'}, {'N': 'A'}, {'C': 'N'}, {'R': 'C'}, {'C': 'R'}, {'N': 'C', 'V': 'C'}, {'N': 'V', 'D': 'V', 'A': 'V'}, {'A': 'D'}, {'O': 'A'}, {'UNK': 'O'}, {'N': 'UNK'}, {'D': 'N', 'C': 'N', 'R': 'N', 'W': 'N'}, {'UNK': 'W'}, {'N': 'UNK'}, {'N': 'N', 'V': 'N'}, {'O': 'N'}]\n",
      "26 2 [{'start|N': -11.921906067830793, 'start|A': -14.930880264495901}, {'N|N': -24.17722975783893}, {'N|O': -26.470831778372293}, {'O|N': -40.89608046982274, 'O|D': -28.31749461302625, 'O|A': -39.651207181500446}, {'D|UNK': -44.96253083922474}, {'UNK|N': -52.87998489047649}, {'N|N': -63.30523358192694}, {'N|O': -65.59883560246031}, {'O|V': -70.74265540870798}, {'V|UNK': -83.88635748556591}, {'UNK|A': -90.82832814936899, 'UNK|R': -91.72007587743578}, {'A|N': -103.08365183937713}, {'N|C': -110.21084014042033}, {'C|R': -119.79301976529422}, {'R|C': -123.52431696067885}, {'C|N': -135.9495656521293, 'C|V': -134.88148404865996}, {'V|N': -152.31718305404212, 'V|D': -139.3938777482197, 'V|A': -151.98001521310988}, {'D|A': -154.4407642854405}, {'A|O': -156.73436630597388}, {'O|UNK': -169.8780683828318}, {'UNK|N': -182.30331707428226}, {'N|D': -189.10931203233574, 'N|C': -187.80357958150063, 'N|R': -195.52935288893087, 'N|W': -188.02452653969814}, {'W|UNK': -205.97687108447414}, {'UNK|N': -217.49522918031607}, {'N|N': -230.92047787176654, 'N|V': -230.17432436318455}, {'N|O': -233.59462656114425}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "K = 2\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "def load_data_set(filename):\n",
    "    # your code\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        pairs = line.split()\n",
    "        current_tokens = []\n",
    "        current_tags = []\n",
    "        for pair in pairs:\n",
    "            data = pair.split('/')\n",
    "            current_tokens.append(data[0].lower())\n",
    "            current_tags.append(data[1])\n",
    "        tokens.append(current_tokens)\n",
    "        tags.append(current_tags)\n",
    "    return tokens, tags\n",
    "\n",
    "def preprocessing(tokens,tags):\n",
    "    k = K\n",
    "    dic = {}\n",
    "    prepocessed_tags = deepcopy(tags)\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            if token in dic:\n",
    "                dic[token] += 1\n",
    "            else:\n",
    "                dic[token] = 1\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if dic[tokens[i][j]] < k:\n",
    "                prepocessed_tags[i][j] = \"UNK\"\n",
    "    return prepocessed_tags,dic\n",
    "\n",
    "def get_probilities(tokens,tags,x_dic):\n",
    "    y_dic = {}\n",
    "    y_y_dic = {}\n",
    "    for tag_set in tags:\n",
    "        new_tag_set = list(tag_set)\n",
    "        new_tag_set.insert(0, 'start')\n",
    "        new_tag_set.append('end')\n",
    "        \n",
    "        for i in range(len(new_tag_set)):\n",
    "            if new_tag_set[i] in y_dic:\n",
    "                y_dic[new_tag_set[i]] += 1\n",
    "            else:\n",
    "                y_dic[new_tag_set[i]] = 1\n",
    "            \n",
    "            if i == len(new_tag_set) - 1:\n",
    "                break\n",
    "            \n",
    "            if new_tag_set[i]+'|'+new_tag_set[i+1] in y_y_dic:\n",
    "                y_y_dic[new_tag_set[i]+'|'+new_tag_set[i+1]] += 1\n",
    "            else:\n",
    "                y_y_dic[new_tag_set[i]+'|'+new_tag_set[i+1]] = 1\n",
    "#     print(y_dic,y_y_dic)\n",
    "    p_y_y = y_y_dic.copy()\n",
    "    for y_y in y_y_dic.keys():\n",
    "        p_y_y[y_y] = 0.0\n",
    "        p_y_y[y_y] = (y_y_dic[y_y]+alpha)/(y_dic[y_y.split('|')[0]]+alpha*(len(y_dic)+1))\n",
    "    \n",
    "    x_y_dic = {}\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if tokens[i][j]+'|'+tags[i][j] in x_y_dic:\n",
    "                x_y_dic[tokens[i][j]+'|'+tags[i][j]] += 1\n",
    "            else:\n",
    "                x_y_dic[tokens[i][j]+'|'+tags[i][j]] = 1\n",
    "    \n",
    "    p_x_y = {}\n",
    "    for x_y in x_y_dic.keys():\n",
    "        p_x_y[x_y] = 0.0\n",
    "        p_x_y[x_y] = (x_y_dic[x_y]+beta)/(y_dic[x_y.split('|')[1]]+beta*len(x_dic))    \n",
    "    \n",
    "#     count = 0.0\n",
    "#     for x_y in p_x_y.keys():\n",
    "#         if x_y.split('|')[1] == 'C':\n",
    "#             count += p_x_y[x_y]\n",
    "#     print(count)\n",
    "    \n",
    "#     count = 0.0\n",
    "#     for y_y in p_y_y.keys():\n",
    "#         if y_y[0] == 'P':\n",
    "#             count += p_y_y[y_y]\n",
    "#     print(count)\n",
    "    return p_y_y,p_x_y,y_dic\n",
    "\n",
    "def to_logistic(p_y_y,p_x_y):\n",
    "    log_p_y_y = p_y_y.copy()\n",
    "    log_p_x_y = p_x_y.copy()\n",
    "    for key in log_p_y_y:\n",
    "        log_p_y_y[key] = math.log2(log_p_y_y[key])\n",
    "    for key in log_p_x_y:\n",
    "        log_p_x_y[key] = math.log2(log_p_x_y[key])\n",
    "    return log_p_y_y,log_p_x_y\n",
    "\n",
    "def Viterbi(log_p_y_y,log_p_x_y,dev_tokens,y_dic):\n",
    "    vs = []\n",
    "    res = []\n",
    "    for scentence in dev_tokens:\n",
    "        v = []\n",
    "        b = []\n",
    "        pre = []\n",
    "        #First loop, v1(k)\n",
    "        v_s = {}\n",
    "        token = scentence[0]\n",
    "        for y_y in log_p_y_y.keys():\n",
    "            if y_y.split('|')[0] == 'start':\n",
    "                v_s[y_y] = log_p_y_y[y_y]\n",
    "                if token+'|'+y_y.split('|')[1] in log_p_x_y:\n",
    "                    v_s[y_y] += log_p_x_y[token+'|'+y_y.split('|')[1]]\n",
    "                else:\n",
    "                    v_s.pop(y_y)\n",
    "        v.append(v_s)\n",
    "        #for 2 to M\n",
    "        pre_v_m = v_s\n",
    "        for token in scentence[1:]:\n",
    "            v_m = {}\n",
    "            b_m = {}\n",
    "            # for k\n",
    "            for tag in y_dic:\n",
    "                # skip the impossible tags\n",
    "                if token + '|' + tag not in log_p_x_y:\n",
    "                    continue\n",
    "                # for each possibile tags, select the most scored previous tag\n",
    "                max_b = list(pre_v_m.keys())[0].split('|')[1]\n",
    "                max_v = list(pre_v_m.values())[0]\n",
    "                for prev in pre_v_m:\n",
    "                    last_y = prev.split('|')[1]\n",
    "                    if pre_v_m[prev] + log_p_y_y[last_y+'|'+tag] > max_v + log_p_y_y[max_b+'|'+tag]:\n",
    "                        max_b = last_y\n",
    "                        max_v = pre_v_m[prev] + log_p_y_y[last_y+'|'+tag]\n",
    "                v_m[max_b+'|'+tag] = max_v + log_p_x_y[token+'|'+tag]\n",
    "                b_m[tag] = max_b\n",
    "            v.append(v_m)\n",
    "            b.append(b_m)\n",
    "            pre_v_m = v_m\n",
    "        vs.append(v)\n",
    "        \n",
    "        #for last\n",
    "        pairs = list(v[-1].keys())\n",
    "        last_tag = pairs[0].split('|')[1]\n",
    "        for pair in pairs[1:]:\n",
    "            cur_tag = pair.split('|')[1]\n",
    "            if log_p_y_y[cur_tag+'|'+'end'] > log_p_y_y[last_tag+'|'+'end']:\n",
    "                last_tag = cur_tag\n",
    "        print(last_tag)\n",
    "        print(b)\n",
    "        \n",
    "        # read the result\n",
    "        prev = last_tag\n",
    "        for pair in reversed(b):\n",
    "            pass #\n",
    "                    \n",
    "        break\n",
    "#                 # for k'\n",
    "#                 max_v = log_p_y_y[log_p_y_y.keys()[0]]\n",
    "#                 max_k = ''\n",
    "#                 for pre_tag in y_dic:\n",
    "#                     s = log_p_x_y[token+'|'+pre_tag] + \n",
    "                    \n",
    "        \n",
    "                \n",
    "        \n",
    "    print(len(v),len(v[0]),vs[0])\n",
    "    \n",
    "\n",
    "trn_tokens, trn_tags = load_data_set(\"./data/dev.pos\")\n",
    "prepocessed_tags,x_dic = preprocessing(trn_tokens,trn_tags)\n",
    "\n",
    "# print(len(trn_texts[0]),len(trn_tags[0]))\n",
    "p_y_y,p_x_y,y_dic = get_probilities(trn_tokens,prepocessed_tags,x_dic)\n",
    "# print(p_x_y)\n",
    "log_p_y_y,log_p_x_y = to_logistic(p_y_y,p_x_y)\n",
    "# print(len(trn_tokens))\n",
    "\n",
    "dev_tokens, dev_tags = load_data_set(\"./data/dev.pos\")\n",
    "Viterbi(log_p_y_y,log_p_x_y,dev_tokens,y_dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
