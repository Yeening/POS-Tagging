{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'V', 'N', 'W', 'P', 'O', 'D', 'N', 'R', 'V', 'R', 'V', 'R', 'C', 'D', 'N', 'O']\n",
      "['R', 'V', 'N', 'V', 'P', 'O', 'D', 'N', 'R', 'V', 'R', 'V', 'R', 'C', 'D', 'N', 'O']\n",
      "0.887682844885512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "K = 2\n",
    "alpha = 0.4\n",
    "beta = 0.3\n",
    "\n",
    "def load_data_set(filename):\n",
    "    # your code\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        pairs = line.split()\n",
    "        current_tokens = []\n",
    "        current_tags = []\n",
    "        for pair in pairs:\n",
    "            data = pair.split('/')\n",
    "            current_tokens.append(data[0].lower())\n",
    "            current_tags.append(data[1])\n",
    "        tokens.append(current_tokens)\n",
    "        tags.append(current_tags)\n",
    "    return tokens, tags\n",
    "\n",
    "def preprocessing(tokens,tags):\n",
    "    k = K\n",
    "    dic = {}\n",
    "    dic[\"UNK\"] = 0\n",
    "#     prepocessed_tags = deepcopy(tags)\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            if token in dic:\n",
    "                dic[token] += 1\n",
    "            else:\n",
    "                dic[token] = 1\n",
    "#     new_dic = {}\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if dic[tokens[i][j]] < k:\n",
    "                ori = tokens[i][j]\n",
    "                dic[\"UNK\"] += dic[ori]\n",
    "                tokens[i][j] = \"UNK\"\n",
    "                dic.pop(ori)\n",
    "                \n",
    "    return tokens,dic\n",
    "\n",
    "def get_probilities(tokens,tags,x_dic):\n",
    "    y_dic = {}\n",
    "    y_y_dic = {}\n",
    "    for tag_set in tags:\n",
    "        new_tag_set = list(tag_set)\n",
    "        new_tag_set.insert(0, 'start')\n",
    "        new_tag_set.append('end')\n",
    "        \n",
    "        for i in range(len(new_tag_set)):\n",
    "            if new_tag_set[i] in y_dic:\n",
    "                y_dic[new_tag_set[i]] += 1\n",
    "            else:\n",
    "                y_dic[new_tag_set[i]] = 1\n",
    "            \n",
    "            if i == len(new_tag_set) - 1:\n",
    "                break\n",
    "            \n",
    "            if new_tag_set[i]+'|'+new_tag_set[i+1] in y_y_dic:\n",
    "                y_y_dic[new_tag_set[i]+'|'+new_tag_set[i+1]] += 1\n",
    "            else:\n",
    "                y_y_dic[new_tag_set[i]+'|'+new_tag_set[i+1]] = 1\n",
    "#     print(y_dic,y_y_dic)\n",
    "    p_y_y = y_y_dic.copy()\n",
    "    for y_y in y_y_dic.keys():\n",
    "        p_y_y[y_y] = 0.0\n",
    "        p_y_y[y_y] = (y_y_dic[y_y]+alpha)/(y_dic[y_y.split('|')[0]]+alpha*(len(y_dic)+1))\n",
    "    \n",
    "    # fill blanks\n",
    "    for y in list(y_dic.keys())[1:]:\n",
    "        if y == 'end':\n",
    "            continue\n",
    "        for y1 in list(y_dic.keys())[1:]:\n",
    "            y_y = y + '|' + y1\n",
    "            if y_y not in p_y_y:\n",
    "                p_y_y[y_y] = (0+alpha)/(y_dic[y]+alpha*(len(y_dic)+1))\n",
    "    \n",
    "    x_y_dic = {}\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if tokens[i][j]+'|'+tags[i][j] in x_y_dic:\n",
    "                x_y_dic[tokens[i][j]+'|'+tags[i][j]] += 1\n",
    "            else:\n",
    "                x_y_dic[tokens[i][j]+'|'+tags[i][j]] = 1\n",
    "    \n",
    "    p_x_y = {}\n",
    "    for x_y in x_y_dic.keys():\n",
    "        p_x_y[x_y] = 0.0\n",
    "        p_x_y[x_y] = (x_y_dic[x_y]+beta)/(y_dic[x_y.split('|')[1]]+beta*len(x_dic))    \n",
    "    \n",
    "    # fill blanks\n",
    "    for x in x_dic:\n",
    "        for y in list(y_dic.keys())[1:]:\n",
    "            if y == 'end':\n",
    "                continue\n",
    "            x_y = x + '|' + y\n",
    "            if x_y not in p_x_y:\n",
    "                p_x_y[x_y] = (0+beta)/(y_dic[y]+beta*len(x_dic))    \n",
    "    \n",
    "#     count = 0.0\n",
    "#     for x_y in p_x_y.keys():\n",
    "#         if x_y.split('|')[1] == 'C':\n",
    "#             count += p_x_y[x_y]\n",
    "#     print(count)\n",
    "    \n",
    "#     count = 0.0\n",
    "#     for y_y in p_y_y.keys():\n",
    "#         if y_y[0] == 'P':\n",
    "#             count += p_y_y[y_y]\n",
    "#     print(count)\n",
    "    return p_y_y,p_x_y,y_dic\n",
    "\n",
    "def to_logistic(p_y_y,p_x_y):\n",
    "    log_p_y_y = p_y_y.copy()\n",
    "    log_p_x_y = p_x_y.copy()\n",
    "    for key in log_p_y_y:\n",
    "        log_p_y_y[key] = math.log2(log_p_y_y[key])\n",
    "    for key in log_p_x_y:\n",
    "        log_p_x_y[key] = math.log2(log_p_x_y[key])\n",
    "    return log_p_y_y,log_p_x_y\n",
    "\n",
    "def Viterbi(log_p_y_y,log_p_x_y,dev_tokens,y_dic):\n",
    "    vs = []\n",
    "    predicts = []\n",
    "    for scentence in dev_tokens:\n",
    "        v = []\n",
    "        b = []\n",
    "        predict = []\n",
    "        #First loop, v1(k)\n",
    "        v_s = {}\n",
    "        token = scentence[0]\n",
    "        for y_y in log_p_y_y.keys():\n",
    "            if y_y.split('|')[0] == 'start':\n",
    "                v_s[y_y] = log_p_y_y[y_y]\n",
    "                if token+'|'+y_y.split('|')[1] in log_p_x_y:\n",
    "                    v_s[y_y] += log_p_x_y[token+'|'+y_y.split('|')[1]]\n",
    "                else:\n",
    "                    v_s.pop(y_y)\n",
    "        v.append(v_s)\n",
    "        #for 2 to M\n",
    "        pre_v_m = v_s\n",
    "        for token in scentence[1:]:\n",
    "            v_m = {}\n",
    "            b_m = {}\n",
    "            # for k\n",
    "            for tag in y_dic:\n",
    "                # skip the impossible tags\n",
    "                if token + '|' + tag not in log_p_x_y:\n",
    "                    continue\n",
    "                # for each possibile tags, select the most scored previous tag\n",
    "#                 print(pre_v_m)\n",
    "                max_b = list(pre_v_m.keys())[0].split('|')[1]\n",
    "                max_v = list(pre_v_m.values())[0]\n",
    "                for prev in pre_v_m:\n",
    "                    last_y = prev.split('|')[1]\n",
    "                    if last_y+'|'+tag not in log_p_y_y:\n",
    "                        continue\n",
    "                    if pre_v_m[prev] + log_p_y_y[last_y+'|'+tag] > max_v + log_p_y_y[max_b+'|'+tag]:\n",
    "                        max_b = last_y\n",
    "                        max_v = pre_v_m[prev] + log_p_y_y[last_y+'|'+tag]\n",
    "                v_m[max_b+'|'+tag] = max_v + log_p_x_y[token+'|'+tag]\n",
    "                b_m[tag] = max_b\n",
    "            v.append(v_m)\n",
    "            b.append(b_m)\n",
    "            pre_v_m = v_m\n",
    "        vs.append(v)\n",
    "        \n",
    "        #for last\n",
    "        pairs = list(v[-1].keys())\n",
    "        last_tag = pairs[0].split('|')[1]\n",
    "        for pair in pairs[1:]:\n",
    "            cur_tag = pair.split('|')[1]\n",
    "            if log_p_y_y[cur_tag+'|'+'end'] > log_p_y_y[last_tag+'|'+'end']:\n",
    "                last_tag = cur_tag\n",
    "#         print(last_tag)\n",
    "#         print(b)\n",
    "        \n",
    "        # read the result\n",
    "        prev = last_tag\n",
    "        predict.append(prev)\n",
    "        for b_m in reversed(b):\n",
    "            predict.append(b_m[prev])\n",
    "            prev = b_m[prev]\n",
    "        predict = [ele for ele in reversed(predict)]\n",
    "        predicts.append(predict)\n",
    "#         print(predict)\n",
    "                    \n",
    "#         break\n",
    "\n",
    "    return predicts\n",
    "                    \n",
    "def get_acc(tags,predicts):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i in range(len(tags)):\n",
    "        total += len(tags[i])\n",
    "        for j in range(len(tags[i])):\n",
    "            if tags[i][j] == predicts[i][j]:\n",
    "                correct += 1\n",
    "    return correct/total\n",
    "    \n",
    "\n",
    "trn_tokens, trn_tags = load_data_set(\"./data/dev.pos\")\n",
    "prepocessed_tokens,x_dic = preprocessing(trn_tokens,trn_tags)\n",
    "# print(len(x_dic))\n",
    "\n",
    "# print(len(trn_texts[0]),len(trn_tags[0]))\n",
    "p_y_y,p_x_y,y_dic = get_probilities(prepocessed_tokens,trn_tags,x_dic)\n",
    "# print(y_dic)\n",
    "# print(p_y_y.keys())\n",
    "# print(len(p_x_y))\n",
    "log_p_y_y,log_p_x_y = to_logistic(p_y_y,p_x_y)\n",
    "# print(len(trn_tokens))\n",
    "\n",
    "dev_tokens, dev_tags = load_data_set(\"./data/dev.pos\")\n",
    "prepocessed_dev_tokens,x_dev_dic = preprocessing(dev_tokens,dev_tags)\n",
    "# a,b,y_dic = get_probilities(prepocessed_dev_tokens,dev_tags,x_dev_dic)\n",
    "dev_predicts = Viterbi(log_p_y_y,log_p_x_y,prepocessed_dev_tokens,y_dic)\n",
    "acc = get_acc(dev_tags,dev_predicts)\n",
    "print(dev_predicts[3])\n",
    "print(dev_tags[3])\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
